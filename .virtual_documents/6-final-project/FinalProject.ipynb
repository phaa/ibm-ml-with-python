








!pip install -q numpy 
!pip install -q pandas
!pip install -q matplotlib
!pip install -q scikit-learn==1.5.0
!pip install -q seaborn
!pip install -q xgboost
# !pip install -q umap-learn


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
#from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score
from xgboost import XGBClassifier
# from umap import UMAP





url="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/_0eYOqji3unP1tDNKWZMjg/weatherAUS-2.csv"
df = pd.read_csv(url)
df.head()





# Get number of entries for each column
df.count()


# Get missing values count
df.isna().sum()


df.describe()


df = df.dropna()
df.isna().sum()





df = df.rename(
    columns={
        'RainToday': 'RainYesterday',
        'RainTomorrow': 'RainToday'
    }
)





loc_mask = df['Location'].isin(['Melbourne', 'MelbourneAirport', 'Watsonia'])
df = df[loc_mask]
df.info()





def plot_boxplots(df, exclude_cols=None):
    if exclude_cols is None:
        exclude_cols = []
        
	# Get only numeric columns
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    # Calculate the number of rows and columns for the subplot
    n_cols = min(3, len(numeric_cols))
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    # Create figure and subplots
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 16))
    
    # Plot each boxplot
    for i, col in enumerate(numeric_cols):
        row_idx = i // n_cols
        col_idx = i % n_cols
        
        # Create the boxplot
        sns.boxplot(y=df[col], ax=axes[row_idx, col_idx], color='cornflowerblue')
        axes[row_idx, col_idx].set_title(f'Boxplot of {col}')
        axes[row_idx, col_idx].set_ylabel('Values')
        
        # Add labels for outliers
        outliers = outliers_column(df[col])
        if len(outliers) > 0:
            axes[row_idx, col_idx].text(0.02, 0.95, f'Outliers: {len(outliers)}', 
                                      transform=axes[row_idx, col_idx].transAxes)
    
    # Remove empty subplots
    for i in range(len(numeric_cols), n_rows * n_cols):
        row_idx = i // n_cols
        col_idx = i % n_cols
        fig.delaxes(axes[row_idx, col_idx])
    
    plt.tight_layout()
    plt.show()
    

def outliers_column(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = column[(column < lower_bound) | (column > upper_bound)]
    return outliers.tolist()



# Plot all coluns except the target one
plot_boxplots(df, exclude_cols=['RainToday'])





def winsorize_outliers(df: pd.DataFrame, cols: list[str]):
	# Create a deep copy of the dataframe to avoid modifying the original
	df_cleaned = df.copy()

	for col_name in cols:
		col_values = df[col_name]

		# Calculate Q1, Q2 and IQR
		q1 = col_values.quantile(0.25)
		q3 = col_values.quantile(0.75)
		iqr = q3 - q1 # Interquartile range

		# Define bounds for outliers
		lower_bound = q1 - (1.5 * iqr)
		upper_bound = q3 + (1.5 * iqr)

		# Values below the lower bound are replaced by the lower bound itself
		df_cleaned.loc[df_cleaned[col_name] < lower_bound, col_name] = lower_bound
        
        # Values above the upper bound are replaced by the upper bound itself
		df_cleaned.loc[df_cleaned[col_name] > upper_bound, col_name] = upper_bound

	return df_cleaned


# Get a new dataframe with winzorized values
df = winsorize_outliers(df, cols=['Rainfall', 'WindGustSpeed', 'Evaporation', 'WindSpeed3pm'])

# Let's take a look at the plots
plot_boxplots(df, exclude_cols=['RainToday'])








def remove_collinear_features(df: pd.DataFrame, columns: list[str], threshold: float):
    # Calculate the correlation matrix
    corr_matrix = df[columns].corr()
    iters = range(len(corr_matrix.columns) - 1)
    drop_cols = []

    # Iterate through the correlation matrix and compare correlations
    for i in iters:
        for j in range(i+1):
            # To get an item via slicing: item = [item_row:item_row+1, item_col:item_col+1]
            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]
            
            col = item.columns # Index object
            row = item.index # Index object
            val = abs(item.values) 

            if val >= threshold:
                # Print the correlated features and the correlation value
                #print(col.values[0], "|", row.values[0], "|", round(val[0][0], 2))
                drop_cols.append(col.values[0])

    # Remove duplicates
    drops = set(drop_cols)
    df = df.drop(columns=drops, axis=1)
    print('Removed Columns {}'.format(drops))
    return df


numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
df = remove_collinear_features(df, numeric_cols, 0.80)

# Inspect the dataframe
df.head()





def date_to_season(date):
    month = date.month
    if (month == 12) or (month == 1) or (month == 2):
        return 'Summer'
    elif month >= 3 and month <= 5:
        return 'Autumn'
    elif month >= 6 and month <= 8:
        return 'Winter'
    elif month >= 9 and month <= 11:
        return 'Spring'


df['Date'] = pd.to_datetime(df['Date'])

# Create Season col
df['Season'] = df['Date'].apply(date_to_season)

# Now drop Date col
df = df.drop(columns=['Date'])

df.sample(5)


# Let's see how distributed is our new feature across the dataset
sns.countplot(x='Season', data=df, color='cornflowerblue')
plt.xlabel('Season')
plt.ylabel('Count')
plt.title('Season Distribution')
plt.xticks(rotation=45)  # Rotate labels for better readability if needed
plt.show()





df["RainToday"] = df["RainToday"].map({"No": 0, "Yes": 1})


X = df.drop(columns=['RainToday'], axis=1)
y = df['RainToday']





sns.countplot(x='RainToday', data=df, color='cornflowerblue')
plt.xlabel('RainToday')
plt.ylabel('Count')
plt.title('RainToday Distribution')
plt.show()





# We must ensure to pass stratify=y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)





# Separate data columns
numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

# Define transformers pipeline for each kind of data
numeric_transformer = Pipeline(steps=[
  	('scaler', StandardScaler()),
  	# ('umap', UMAP(n_components=10, random_state=42)) # Bad results with UMAP
])
categorical_transformer = Pipeline(steps=[
  	('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers into a single preprocessing ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

classifier = RandomForestClassifier(class_weight='balanced', random_state=42)

# Create a pipeline by combining the preprocessor with a RandomForest model
model_pipeline = Pipeline(steps=[
  	('preprocessor', preprocessor),
  	('SMOTE', SMOTE()),
  	('classifier', classifier) 
])





# Define a param grid to use during cross-validation steps
param_grid = {
	'classifier__n_estimators': [300, 400, 500], # Num of trees
    'classifier__max_depth': [None, 10, 20, 30], # Max depth of each tree
    'classifier__min_samples_split': [2, 5, 10], # Min of samples to split a tree node 
    'classifier__min_samples_leaf': [1, 2, 4], # Min of samples per leaf
    'classifier__bootstrap': [True], # Use bootstrap sampling 
    'classifier__max_features': ['sqrt', 'log2'] # Num of features to be use in each node split
}

# Select cross-validation method
cv = StratifiedKFold(n_splits=5, shuffle=True)

# Instantiate GridSearchCV using our pipeline
grid_search = GridSearchCV(model_pipeline, param_grid, cv=cv, scoring='f1', verbose=2, n_jobs=-1)

grid_search.fit(X_train, y_train)





def print_scores(X_test, y_test, grid_search: GridSearchCV):
	# Get best scoring during cross-validation
	best_cv_score = grid_search.best_score_
	print('Best training score: {:.2f}'.format(best_cv_score))

	# Now get scoring for test data
	test_score = grid_search.score(X_test, y_test)
	print('Test set score: {:.2f}'.format(test_score))

	print('Best parameters: ', grid_search.best_params_)


print_scores(X_test, y_test, grid_search)








# Function for plotting both confusion matrix and classification report
def plot_conf_matrix_report(X_test, y_test, grid_search: GridSearchCV):
	# Get the best estimator and do predictions on unseen data
	y_pred = grid_search.best_estimator_.predict(X_test)

	clf_report = classification_report(y_test, y_pred, labels=['0', '1'], output_dict=True)
	report_df = pd.DataFrame(clf_report).iloc[:-1, :].T # .iloc[:-1, :] to exclude support
	sns.heatmap(report_df, annot=True)

	conf_matrix = confusion_matrix(y_test, y_pred)
	disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
	disp.plot(cmap='Blues')
	plt.title('Confusion Matrix')
	plt.show();


def plot_conf_matrix_report_2(y_pred, y_test):
	clf_report = classification_report(y_test, y_pred, labels=['0', '1'], output_dict=True)
	report_df = pd.DataFrame(clf_report).iloc[:-1, :].T # .iloc[:-1, :] to exclude support
	sns.heatmap(report_df, annot=True)

	conf_matrix = confusion_matrix(y_test, y_pred)
	disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
	disp.plot(cmap='Blues')
	plt.title('Confusion Matrix')
	plt.show();



plot_conf_matrix_report(X_test, y_test, grid_search)





def preprocess_data(df: pd.DataFrame, target_col: str):
    # Remove missing values
    df = df.dropna()

    # Rename columns for clarity
    df = df.rename(columns={'RainToday': 'RainYesterday', 'RainTomorrow': 'RainToday'})

    # Filter for specific locations
    locations = ['Melbourne', 'MelbourneAirport', 'Watsonia']
    df = df[df['Location'].isin(locations)]

    # Convert Date column to datetime format
    df['Date'] = pd.to_datetime(df['Date'])

    # Create a new column for seasons based on the date
    df['Season'] = df['Date'].apply(date_to_season)

    # Drop the Date column as it is no longer needed
    df = df.drop(columns=['Date'])

    # Convert categorical values to numerical format
    df["RainToday"] = df["RainToday"].map({"No": 0, "Yes": 1})
    df["RainYesterday"] = df["RainYesterday"].map({"No": 0, "Yes": 1})

    # Split into features (X) and target variable (y)
    X = df.drop(columns=[target_col])
    y = df[target_col]

    return df, X, y


# Get dataset from url
df = pd.read_csv(url)

# Preprocess data and create feature and target sets
df, X, y = preprocess_data(df, target_col='RainToday')

X.sample(10)


# Split train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Get column names
numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Update our pipeline
model_pipeline.set_params(preprocessor=preprocessor)

# Retrain only with the top features
grid_search.fit(X_train, y_train)





print_scores(X_test, y_test, grid_search)





plot_conf_matrix_report(X_test, y_test, grid_search)





def get_feature_importances(grid_search: GridSearchCV):
	# Associate feature importances with their original input variables
	cat_features_backward = list(
		grid_search.best_estimator_['preprocessor']
		.named_transformers_['cat']
		.named_steps['onehot']
		.get_feature_names_out(categorical_features)
	)

	num_features_backward = list(
		grid_search.best_estimator_['preprocessor']
		.named_transformers_['num']
		.get_feature_names_out()
	)

	# Combine numeric and categorical feature names
	feature_names = num_features_backward + cat_features_backward

	feature_importances = grid_search.best_estimator_['classifier'].feature_importances_
	importance_df = pd.DataFrame({
		'Feature': feature_names,
		'Importance': feature_importances
	})
	
	importance_df['Feature'] = importance_df['Feature'].str.split('_').str[0]
	importance_df = importance_df.groupby('Feature', as_index=False).sum()
	importance_df = importance_df.sort_values(by='Importance', ascending=False)

	return importance_df


def plot_feature_importances(features, importances, title='Features importances'):
	# Plot importances
	plt.figure(figsize=(10, 6))
	plt.barh(features, importances, color='cornflowerblue')
	plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top
	plt.title(title)
	plt.xlabel('Importance Score')
	plt.show();






importance_df = get_feature_importances(grid_search)
plot_feature_importances(importance_df['Feature'], importance_df['Importance'])








threshold = 0.85 

importance_df["Cumulative Importance"] = np.cumsum(importance_df["Importance"])

plt.figure(figsize=(10, 6))
plt.plot(
	importance_df["Feature"], 
	importance_df["Cumulative Importance"], 
	marker='o', 
	linestyle='-', 
	color='cornflowerblue', 
	label="Cumulative Importance"
)

# Line for threshold
plt.axhline(y=threshold, color='indianred', linestyle='--', label="85% Threshold")
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Cumulative Importance")
plt.title("Cumulative Feature Importance")
plt.legend()
plt.grid()

plt.show()


# Since the 'importance_df_xgb' DataFrame is already sorted by importance,  
# we compute the cumulative sum of feature importance scores.  
cumulative_importance = np.cumsum(importance_df['Importance'])  

# Find the index of the first feature where the cumulative sum exceeds the defined threshold.  
threshold_idx = np.where(cumulative_importance > threshold)[0][0]  

# Select the most important features 
top_features = importance_df[:threshold_idx + 1]  
n_selected_features = len(top_features)  

top_features.head(n_selected_features)  


plot_feature_importances(
  	top_features['Feature'], 
  	top_features['Importance'],
    title=f'Top {n_selected_features} Most Important Features'
)





X_tf = df[top_features['Feature']]
y_tf = df['RainToday']

X_tf.head()


X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, y_tf, test_size=0.2, stratify=y, random_state=42)

# Get column names
numeric_features_tf = X_train_tf.select_dtypes(include=['number']).columns.tolist()
categorical_features_tf = X_train_tf.select_dtypes(include=['object', 'category']).columns.tolist()

numeric_transformer_tf = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer_tf = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor_tf = ColumnTransformer(transformers=[
    ('num', numeric_transformer_tf, numeric_features_tf),
    ('cat', categorical_transformer_tf, categorical_features_tf)
])

# Update our pipeline
model_pipeline.set_params(preprocessor=preprocessor_tf)

# Retrain only with the top features
grid_search.fit(X_train_tf, y_train_tf)






print_scores(X_test_tf, y_test_tf, grid_search)


plot_conf_matrix_report(X_test_tf, y_test_tf, grid_search)





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

results = {}

model_pipeline.set_params(**grid_search.best_params_)

for i in range(3, len(importance_df)): # Start using at least 3 features
	selected_features = importance_df['Feature'][:i]

	X_train_selected = X_train.loc[:, selected_features]
	X_test_selected = X_test.loc[:, selected_features]

	# Each time we try our model with different cols, we need to update the pipeline transformers
	numeric_features = X_train_selected.select_dtypes(include=['number']).columns.tolist()
	categorical_features = X_train_selected.select_dtypes(include=['object', 'category']).columns.tolist()

	numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
	categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

	preprocessor = ColumnTransformer(transformers=[
		('num', numeric_transformer, numeric_features),
		('cat', categorical_transformer, categorical_features)
	])

	# Update our pipeline
	model_pipeline.set_params(preprocessor=preprocessor)

	# We don't need grid search as we already know the best model params
	model_pipeline.fit(X_train_selected, y_train)

	y_pred = model_pipeline.predict(X_test_selected)
	f1 = f1_score(y_test, y_pred) 

	results[i] = f1


best_count_features = max(results, key=results.get)
best_score = results[best_count_features]	


plt.figure(figsize=(8, 5))
plt.plot(results.keys(), results.values(), marker='o', linestyle='-', color='indianred')

plt.xlabel("Features count")
plt.ylabel("Score")
plt.title("Performance vs. Best features count")
plt.grid(True)
plt.show()





top_features = importance_df['Feature'][:5]

# Create the training and test datasets with the top 5 features
X_train_best = X_train.loc[:, top_features]
X_test_best = X_test.loc[:, top_features]

# Update the pipeline transformers
numeric_features = X_train_best.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train_best.select_dtypes(include=['object', 'category']).columns.tolist()

numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Update our model pipeline with the best params and preprocessor
model_pipeline.set_params(**grid_search.best_params_)
model_pipeline.set_params(preprocessor=preprocessor)

model_pipeline.fit(X_train_best, y_train)


# Predict and evaluate the model
y_pred = model_pipeline.predict(X_test_best)
plot_conf_matrix_report_2(y_pred, y_test)








# Get dataset from url
df = pd.read_csv(url)

# Preprocess data and create feature and target sets
df, X, y = preprocess_data(df, target_col='RainToday')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)


# Create feature transformers for XGBoost
numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

xgb_preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])





# Create XGBoost Model
ratio = df["RainToday"].value_counts()[0] / df["RainToday"].value_counts()[1]
xgb_model = XGBClassifier(scale_pos_weight=ratio, random_state=42)

xgb_pipeline = Pipeline(steps=[
  	('preprocessor', xgb_preprocessor),
  	('classifier', xgb_model) 
])


# Create a new grid search for XGBoost
param_grid_xgb = {
  	'classifier__scale_pos_weight': [1, ratio / 2, ratio, ratio * 2, ratio * 5],
    "classifier__n_estimators": [200, 300, 400, 500],
    "classifier__max_depth": [3, 6, 9],
    "classifier__learning_rate": [0.1]
}

cv = StratifiedKFold(n_splits=5, shuffle=True)

# Instantiate GridSearchCV using our pipeline
xgb_grid = GridSearchCV(xgb_pipeline, param_grid_xgb, cv=cv, scoring='f1', verbose=2, n_jobs=-1)

xgb_grid.fit(X_train, y_train)





print_scores(X_test, y_test, xgb_grid)


plot_conf_matrix_report(X_test, y_test, xgb_grid)





importance_df_xgb = get_feature_importances(xgb_grid)
plot_feature_importances(importance_df_xgb['Feature'], importance_df_xgb['Importance'])



